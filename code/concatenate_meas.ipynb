{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create session-level measurements\n",
    "\n",
    "This notebook concatenates all of the measurements collected on various dates into one csv per site. The metrics consist of the following measures, with one summary statistic per session-treatment group:\n",
    "\n",
    "- indoor air temperature (mean, std dev, quartiles)\n",
    "- indoor relative humidity (mean, std dev, quartiles)\n",
    "- outdoor air temperature, avg/min/max over the date of each session.\n",
    "- outdoor relative humidity, avg/min/max over the date of each session.\n",
    "- participant level mean T and RH, averaged over duration of session.\n",
    "- operative temp in control and tx\n",
    "- CO2 in control and tx\n",
    "\n",
    "This also aggregates the module timing data and saves it in a separate file.\n",
    "\n",
    "These files are used for plotting in the separate `publication_figures_and_tables.ipynb` notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import timedelta\n",
    "from os.path import basename\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from utilities import (\n",
    "    Settings,\n",
    "    get_timing_df_berk,\n",
    "    get_timing_df_bus,\n",
    "    load_vals_berkeley,\n",
    "    load_vals_bus,\n",
    ")\n",
    "\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "home_dir_s = (\n",
    "    \"/Users/ianbolliger/Library/CloudStorage/Dropbox-Personal/Temperature & Behavior/\"\n",
    "    \"Replication preparation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = Path(home_dir_s)\n",
    "s = Settings(home_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_session_level_indoor_vals(dfs, timing_df, site_settings):\n",
    "    out_df = pd.DataFrame()\n",
    "    idx = pd.IndexSlice\n",
    "\n",
    "    for grp_ix, grp in enumerate([\"control\", \"treatment\"]):\n",
    "        grp_df = timing_df[timing_df[\"Treatment group\"] == grp_ix]\n",
    "\n",
    "        # one dataframe for each sensor location\n",
    "        loc_dfs = []\n",
    "        for loc in (\n",
    "            site_settings.sens_locs + [str(i) for i in range(1, 7)] + [\"Top\", \"co2\"]\n",
    "        ):\n",
    "            tmp_df = dfs[grp][loc]\n",
    "            tmp_df[\"sess\"] = pd.cut(tmp_df.index, bins=grp_df.index)\n",
    "            grouped = tmp_df.groupby(\"sess\", observed=True).describe()\n",
    "\n",
    "            # drop pilot sessions where we don't have room-level sensors (only\n",
    "            # individual)\n",
    "            grouped = grouped[grouped.loc[:, idx[:, \"count\"]].sum(axis=1) != 0]\n",
    "\n",
    "            # don't need # of measurements taken in session\n",
    "            grouped = grouped.drop(columns=\"count\", level=1)\n",
    "\n",
    "            loc_dfs.append(grouped)\n",
    "\n",
    "        # estimate average values for sessions where only one sensor recording\n",
    "        # then average the sensors for each session\n",
    "        room_sesh_vals = average_two_sensors(loc_dfs)\n",
    "\n",
    "        # clarify that these are indoor temps and RH vals\n",
    "        def renamer(x):\n",
    "            if x == \"one_sensor_only\":\n",
    "                return \"one_sensor_only_in\"\n",
    "            return x.split(\"_\")[0] + \"_in_\" + x.split(\"_\")[1]\n",
    "\n",
    "        sesh_vals = room_sesh_vals.rename(renamer, axis=1)\n",
    "\n",
    "        # add participant-level sensors\n",
    "        for s_ix in range(1, 7):\n",
    "            this_df = loc_dfs[s_ix + 1]\n",
    "\n",
    "            # convert to single-level index\n",
    "            this_df.columns = [\n",
    "                (j[0] + \"_p{}_\".format(s_ix) + j[1]).rstrip(\"_\")\n",
    "                for j in this_df.columns.values\n",
    "            ]\n",
    "\n",
    "            # join onto sesh_vals dataframes\n",
    "            sesh_vals = sesh_vals.join(this_df, how=\"outer\")\n",
    "\n",
    "        # add operative temp\n",
    "        this_df = loc_dfs[8]\n",
    "        this_df.columns = [\n",
    "            (j[0] + \"_\" + j[1]).rstrip(\"_\") for j in this_df.columns.values\n",
    "        ]\n",
    "        sesh_vals = sesh_vals.join(this_df, how=\"outer\")\n",
    "\n",
    "        # add CO2\n",
    "        this_df = loc_dfs[9]\n",
    "        this_df.columns = [\n",
    "            (j[0] + \"_\" + j[1]).rstrip(\"_\") for j in this_df.columns.values\n",
    "        ]\n",
    "        sesh_vals = sesh_vals.join(this_df, how=\"outer\")\n",
    "\n",
    "        # join onto treatment group / session # data\n",
    "        sesh_vals = sesh_vals.join(grp_df, how=\"inner\")\n",
    "        out_df = pd.concat((out_df, sesh_vals))\n",
    "\n",
    "    # format nicely\n",
    "    out_df = out_df.rename(\n",
    "        columns={\n",
    "            \"Date\": \"date\",\n",
    "            \"Session in day\": \"session\",\n",
    "            \"Treatment group\": \"treatment\",\n",
    "        }\n",
    "    )\n",
    "    out_df = out_df.set_index([\"date\", \"session\", \"treatment\"])\n",
    "    out_df = out_df.drop(columns=[\"start_time\", \"end_time\"])\n",
    "    out_df.columns = [c.rstrip(\"%\") for c in out_df.columns]\n",
    "    out_df = out_df.sort_index()\n",
    "    out_df = out_df[out_df.notnull().any(axis=1)]\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def average_two_sensors(loc_dfs):\n",
    "    loc_dfs = bias_correct_one_sensor(loc_dfs)\n",
    "    sesh_vals = (loc_dfs[0] + loc_dfs[1]) / 2\n",
    "    sesh_vals[\"one_sensor_only\"] = pd.DataFrame(\n",
    "        [loc_dfs[0][\"one_sensor_only\"], loc_dfs[1][\"one_sensor_only\"]]\n",
    "    ).max()\n",
    "    return sesh_vals\n",
    "\n",
    "\n",
    "def bias_correct_one_sensor(loc_dfs):\n",
    "    \"\"\"When only one sensor in a room, bias correct that sensor to estimate\n",
    "    the average of the two sensors for that session.\"\"\"\n",
    "    # find mean difference in sensor\n",
    "    mean_diff = (loc_dfs[1] - loc_dfs[0]).mean()\n",
    "    # flatten index\n",
    "    mean_diff.index = [(i[0] + \"_\" + i[1]).rstrip(\"_\") for i in mean_diff.index.values]\n",
    "\n",
    "    # get df with all sessions where at least one sensor was working\n",
    "    all_locs = loc_dfs[1].join(loc_dfs[0], rsuffix=\"0\", lsuffix=\"1\")\n",
    "\n",
    "    # adjust for one-sensor-only times\n",
    "    # workaround b/c reindex fails with intervalIndex (bug)\n",
    "    for i in [0, 1]:\n",
    "        loc_dfs[i] = all_locs.loc[\n",
    "            :, idx[[k for k in all_locs.columns.levels[0] if k[-1] == str(i)], :]\n",
    "        ]\n",
    "\n",
    "        # convert to single-level index\n",
    "        loc_dfs[i] = loc_dfs[i].rename(lambda x: x[:-1], axis=1, level=0)\n",
    "        loc_dfs[i].columns = [\n",
    "            (j[0] + \"_\" + j[1]).rstrip(\"_\") for j in loc_dfs[i].columns.values\n",
    "        ]\n",
    "\n",
    "        # mark where we did bias correction\n",
    "        loc_dfs[i][\"one_sensor_only\"] = loc_dfs[i].isnull().any(axis=1)\n",
    "\n",
    "    loc_dfs[1].iloc[:, :-1] = (\n",
    "        loc_dfs[1]\n",
    "        .iloc[:, :-1]\n",
    "        .where(~loc_dfs[1][\"one_sensor_only\"], loc_dfs[0].iloc[:, :-1] + mean_diff)\n",
    "    )\n",
    "    loc_dfs[0].iloc[:, :-1] = (\n",
    "        loc_dfs[0]\n",
    "        .iloc[:, :-1]\n",
    "        .where(~loc_dfs[0][\"one_sensor_only\"], loc_dfs[1].iloc[:, :-1] - mean_diff)\n",
    "    )\n",
    "\n",
    "    return loc_dfs\n",
    "\n",
    "\n",
    "def add_outdoor_vals(output_df, dfs_outdoor, timing_df):\n",
    "    res = output_df.copy()\n",
    "    for i in [\"min\", \"mean\", \"max\"]:\n",
    "        to_join = dfs_outdoor[i].copy()\n",
    "        to_join.index = pd.to_datetime(to_join.index)\n",
    "        to_join = (\n",
    "            timing_df.join(to_join, on=\"Date\", how=\"inner\")\n",
    "            .drop_duplicates()\n",
    "            .set_index([\"Date\", \"Session in day\", \"Treatment group\"])\n",
    "            .loc[:, [\"T\", \"RH\"]]\n",
    "        )\n",
    "        to_join.columns = [r + \"_out_daily\" + i for r in to_join.columns]\n",
    "        to_join.index.names = res.index.names\n",
    "        res = res.join(to_join, how=\"outer\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berkeley\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading temps: 2017-10-02 00:00:00...\n",
      "Downloading temps: 2017-10-11 00:00:00...\n",
      "Downloading temps: 2017-11-05 00:00:00...\n",
      "Downloading temps: 2017-11-16 00:00:00...\n",
      "Downloading temps: 2017-11-30 00:00:00...\n",
      "Downloading temps: 2017-12-03 00:00:00...\n",
      "Downloading temps: 2017-12-19 00:00:00...\n",
      "Downloading temps: 2018-01-26 00:00:00...\n",
      "Downloading temps: 2018-02-17 00:00:00...\n",
      "Downloading co2: 2017-11-16 00:00:00...\n",
      "Downloading co2: 2017-11-30 00:00:00...\n",
      "Downloading co2: 2017-12-03 00:00:00...\n",
      "Downloading co2: 2017-12-19 00:00:00...\n",
      "Downloading co2: 2018-02-17 00:00:00...\n",
      "Downloading temps: 2017-10-11 00:00:00...\n",
      "Downloading temps: 2017-11-05 00:00:00...\n",
      "Downloading temps: 2017-11-16 00:00:00...\n",
      "Downloading temps: 2017-11-30 00:00:00...\n",
      "Downloading temps: 2017-12-03 00:00:00...\n",
      "Downloading temps: 2017-12-19 00:00:00...\n",
      "Downloading temps: 2018-01-26 00:00:00...\n",
      "Downloading temps: 2018-02-17 00:00:00...\n",
      "Downloading co2: 2017-11-16 00:00:00...\n",
      "Downloading co2: 2017-11-30 00:00:00...\n",
      "Downloading co2: 2017-12-03 00:00:00...\n",
      "Downloading co2: 2017-12-19 00:00:00...\n",
      "Downloading co2: 2018-02-17 00:00:00...\n"
     ]
    }
   ],
   "source": [
    "# load timing data\n",
    "timing_df_berk = get_timing_df_berk(s)\n",
    "\n",
    "# load all raw data\n",
    "dfs_berk = load_vals_berkeley(s)\n",
    "\n",
    "# save aggregated raw data for future loading\n",
    "with (s.clean_data_dir / \"sensor_data_berk.pickle\").open(\"wb\") as f:\n",
    "    pickle.dump(dfs_berk, f)\n",
    "\n",
    "# get session-level aggregate statistics\n",
    "out_df_berk = calc_session_level_indoor_vals(dfs_berk[\"indoor\"], timing_df_berk, s.berk)\n",
    "out_df_berk = add_outdoor_vals(out_df_berk, dfs_berk[\"outdoor\"], timing_df_berk)\n",
    "\n",
    "# save aggregate data\n",
    "out_df_berk.to_csv(s.berk.env_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading control room data...\n",
      "Downloading 20180208...\n",
      "Downloading far sensor...\n",
      "Downloading near sensor...\n",
      "Downloading treatment room data...\n",
      "Downloading 20180208...\n",
      "Downloading far sensor...\n",
      "Downloading near sensor...\n"
     ]
    }
   ],
   "source": [
    "# load timing dataframe\n",
    "timing_df_bus = get_timing_df_bus(s)\n",
    "\n",
    "# load values dataframe\n",
    "dfs_bus = load_vals_bus(s)\n",
    "\n",
    "# save raw data for future loading\n",
    "with (s.clean_data_dir / \"sensor_data_bus.pickle\").open(\"wb\") as f:\n",
    "    pickle.dump(dfs_bus, f)\n",
    "\n",
    "# get session-level aggregate statistics\n",
    "out_df_bus = calc_session_level_indoor_vals(dfs_bus[\"indoor\"], timing_df_bus, s.bus)\n",
    "out_df_bus = add_outdoor_vals(out_df_bus, dfs_bus[\"outdoor\"], timing_df_bus)\n",
    "\n",
    "# save aggregate data\n",
    "out_df_bus.to_csv(s.bus.env_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Timing Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframes of T timeseries by session\n",
    "\n",
    "ts_dfs = {}\n",
    "\n",
    "in_dfs = {\"California\": dfs_berk[\"indoor\"], \"Nairobi\": dfs_bus[\"indoor\"]}\n",
    "timing_dfs = {\"California\": timing_df_berk, \"Nairobi\": timing_df_bus}\n",
    "site_settings = {\"California\": s.berk, \"Nairobi\": s.bus}\n",
    "for site in s.sites:\n",
    "    ts_dfs[site] = {}\n",
    "    timing_df = timing_dfs[site]\n",
    "\n",
    "    for grp_ix, grp in enumerate(s.txs):\n",
    "        grp_df = timing_df[timing_df[\"Treatment group\"] == grp_ix]\n",
    "        ts_dfs[site][grp] = {}\n",
    "\n",
    "        # one dataframe for each sensor location\n",
    "        loc_dfs = []\n",
    "        for loc in (\n",
    "            site_settings[site].sens_locs\n",
    "            + [str(i) for i in range(1, 7)]\n",
    "            + [\"Top\", \"co2\"]\n",
    "        ):\n",
    "            tmp_df = in_dfs[site][grp][loc].reset_index()\n",
    "\n",
    "            # assign to session\n",
    "            tmp_df[\"sess\"] = pd.cut(tmp_df[\"time\"], bins=grp_df.index)\n",
    "\n",
    "            # keep only measurements during sessions\n",
    "            tmp_df = tmp_df[tmp_df[\"sess\"].notnull()]\n",
    "\n",
    "            # get interval for each observation\n",
    "            tmp_df[\"t_ix_sess\"] = tmp_df.groupby(\"sess\", observed=True).cumcount()\n",
    "\n",
    "            # adjust for when we had 5 min intervals in berkeley\n",
    "            if loc in site_settings[site].sens_locs:\n",
    "                tmp_df[\"t_ix_sess\"] = tmp_df[\"t_ix_sess\"].where(\n",
    "                    tmp_df[\"time\"] >= s.berk.sensor_swap_date, tmp_df[\"t_ix_sess\"] * 5\n",
    "                )\n",
    "\n",
    "            # pivot to wide table\n",
    "            if loc == \"Top\":\n",
    "                this_val = [\"Top\"]\n",
    "            elif loc == \"co2\":\n",
    "                this_val = [\"co2\"]\n",
    "            else:\n",
    "                this_val = [\"T\", \"RH\"]\n",
    "            for t in this_val:\n",
    "                val_by_sess = tmp_df.pivot(\n",
    "                    index=\"sess\", columns=\"t_ix_sess\", values=t\n",
    "                ).T\n",
    "                loc_dfs.append(val_by_sess)\n",
    "\n",
    "        # ignoring small # of sessions where only one sensor was in room\n",
    "        combined_cols = loc_dfs[0].columns.intersection(loc_dfs[2].columns)\n",
    "        mean_df = (\n",
    "            loc_dfs[0].loc[:, combined_cols] + loc_dfs[2].loc[:, combined_cols]\n",
    "        ) / 2\n",
    "        ts_dfs[site][grp][\"room_avg_T\"] = mean_df\n",
    "        mean_df_RH = (\n",
    "            loc_dfs[1].loc[:, combined_cols] + loc_dfs[3].loc[:, combined_cols]\n",
    "        ) / 2\n",
    "        ts_dfs[site][grp][\"room_avg_RH\"] = mean_df_RH\n",
    "        for i in range(1, 7):\n",
    "            ts_dfs[site][grp][\"p{}_T\".format(i)] = loc_dfs[2 * (i + 1)]\n",
    "            ts_dfs[site][grp][\"p{}_RH\".format(i)] = loc_dfs[2 * (i + 1) + 1]\n",
    "        ts_dfs[site][grp][\"T_op\"] = loc_dfs[16]\n",
    "        ts_dfs[site][grp][\"CO2\"] = loc_dfs[17]\n",
    "\n",
    "with s.ts_dfs_path.open(\"wb\") as f:\n",
    "    pickle.dump(ts_dfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module time data: /Users/ianbolliger/Library/CloudStorage/Dropbox-Personal/Temperature & Behavior/Replication preparation/data/main/raw/xlab/modules/9-25/Session 2 T\n"
     ]
    }
   ],
   "source": [
    "modules = [\n",
    "    \"production\",\n",
    "    \"dictator\",\n",
    "    \"risk_game\",\n",
    "    \"t_preference\",\n",
    "    \"trust\",\n",
    "    \"public_goods\",\n",
    "    \"ravens\",\n",
    "    \"joy_O_D\",\n",
    "    \"survey\",\n",
    "    \"charity\",\n",
    "]\n",
    "\n",
    "tzs = {\"California\": \"US/Pacific\", \"Nairobi\": \"Africa/Nairobi\"}\n",
    "\n",
    "\n",
    "def get_mean_temp(row, temps_df, st_time, loc, tx):\n",
    "    st = int((row[\"start_time\"] - st_time).total_seconds() / 60)\n",
    "    end = int((row[\"end_time\"] - st_time).total_seconds() / 60)\n",
    "\n",
    "    if st < 0:\n",
    "        # There are many cases in Nairobi where the start time for production\n",
    "        # module occurs before the recorded starting time in the experiment_timing.csv\n",
    "        # file. IB is investigating this w/ Ray but the hunch is that the Busara team\n",
    "        # opened the experiment to the introduction page prior to participants entering\n",
    "        # room. This is b/c the time spent on the intro page is often very long. So we\n",
    "        # are assuming the start times are 0 for these cases.\n",
    "        st = 0\n",
    "    return pd.Series([temps_df[st:end].mean(), st, end], index=[\"temps\", \"st\", \"end\"])\n",
    "\n",
    "\n",
    "module_temps = {\n",
    "    \"California\": {\n",
    "        \"control\": pd.DataFrame(index=modules),\n",
    "        \"treatment\": pd.DataFrame(index=modules),\n",
    "    },\n",
    "    \"Nairobi\": {\n",
    "        \"control\": pd.DataFrame(index=modules),\n",
    "        \"treatment\": pd.DataFrame(index=modules),\n",
    "    },\n",
    "}\n",
    "start_times = {\n",
    "    \"California\": {\n",
    "        \"control\": pd.DataFrame(index=modules),\n",
    "        \"treatment\": pd.DataFrame(index=modules),\n",
    "    },\n",
    "    \"Nairobi\": {\n",
    "        \"control\": pd.DataFrame(index=modules),\n",
    "        \"treatment\": pd.DataFrame(index=modules),\n",
    "    },\n",
    "}\n",
    "end_times = {\n",
    "    \"California\": {\n",
    "        \"control\": pd.DataFrame(index=modules),\n",
    "        \"treatment\": pd.DataFrame(index=modules),\n",
    "    },\n",
    "    \"Nairobi\": {\n",
    "        \"control\": pd.DataFrame(index=modules),\n",
    "        \"treatment\": pd.DataFrame(index=modules),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "for loc in s.sites:\n",
    "    date_dirs = getattr(s, s.site_map[loc]).modules_dir.glob(\"[!.]*\")\n",
    "    for d in date_dirs:\n",
    "        sessions = d.glob(\"[!.]*\")\n",
    "        for se in sessions:\n",
    "            sess = basename(se).split()\n",
    "            if sess[2] == \"C\":\n",
    "                tx = \"control\"\n",
    "            elif sess[2] == \"T\":\n",
    "                tx = \"treatment\"\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            ts_fpath = list(se.glob(\"TimeSpent*\"))\n",
    "            if len(ts_fpath):\n",
    "                ts_fpath = ts_fpath[0]\n",
    "            else:\n",
    "                print(\"No module time data: {}\".format(se))\n",
    "                continue\n",
    "\n",
    "            this_df = pd.read_csv(ts_fpath, index_col=\"time_stamp\")\n",
    "            this_df.index = pd.to_datetime(this_df.index, unit=\"s\")\n",
    "            this_df = (\n",
    "                this_df.tz_localize(\"UTC\")\n",
    "                .tz_convert(tzs[loc])\n",
    "                .tz_localize(None)\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            this_df = this_df.loc[\n",
    "                this_df[\"app_name\"].isin(modules),\n",
    "                [\n",
    "                    \"app_name\",\n",
    "                    \"participant__id_in_session\",\n",
    "                    \"time_stamp\",\n",
    "                    \"seconds_on_page\",\n",
    "                ],\n",
    "            ]\n",
    "            this_df = this_df.sort_values([\"participant__id_in_session\", \"app_name\"])\n",
    "            midtime = this_df.sort_values(\"time_stamp\").iloc[\n",
    "                int(this_df.shape[0] / 2), 2\n",
    "            ]\n",
    "            groups = this_df.groupby([\"participant__id_in_session\", \"app_name\"])\n",
    "\n",
    "            # start times\n",
    "            st = groups.first()\n",
    "            # For production, take starting point as \"end of intro slide\"\n",
    "            # b/c this comes first and in busara they navigated to this page\n",
    "            # before the participants entered\n",
    "            page_start = st[\"time_stamp\"] - st[\"seconds_on_page\"].apply(\n",
    "                lambda x: timedelta(seconds=x)\n",
    "            )\n",
    "            st = page_start.where(\n",
    "                st.index.get_level_values(1) != \"production\", st[\"time_stamp\"]\n",
    "            )\n",
    "            st.name = \"start_time\"\n",
    "\n",
    "            # end times\n",
    "            end = groups.last()[\"time_stamp\"]\n",
    "            end.name = \"end_time\"\n",
    "\n",
    "            # get times for temp data\n",
    "            try:\n",
    "                this_temps = ts_dfs[loc][tx][\"room_avg_T\"][midtime].interpolate()\n",
    "                sess_st_time = this_temps.name.left\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            this_times = pd.DataFrame([st, end]).T\n",
    "            vals = this_times.apply(\n",
    "                lambda x: get_mean_temp(x, this_temps, sess_st_time, loc, tx), axis=1\n",
    "            )\n",
    "            vals = vals[vals[\"temps\"].notnull()]\n",
    "\n",
    "            # average accross participants\n",
    "            vals = vals.groupby(level=1).median()\n",
    "            module_temps[loc][tx][this_temps.name] = vals[\"temps\"]\n",
    "            start_times[loc][tx][this_temps.name] = vals[\"st\"]\n",
    "            end_times[loc][tx][this_temps.name] = vals[\"end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "ix = pd.MultiIndex.from_product(\n",
    "    [s.sites, s.txs, modules], names=[\"location\", \"group\", \"module\"]\n",
    ")\n",
    "t_mod_df = pd.DataFrame(\n",
    "    index=ix, columns=[\"st_time\", \"end_time\", \"mean_temp\"]\n",
    ").sort_index()\n",
    "for loc in s.sites:\n",
    "    for tx in s.txs:\n",
    "        this_df = pd.DataFrame(\n",
    "            {\n",
    "                \"st_time\": start_times[loc][tx].median(axis=1),\n",
    "                \"end_time\": end_times[loc][tx].median(axis=1),\n",
    "                \"mean_temp\": module_temps[loc][tx].median(axis=1),\n",
    "            }\n",
    "        )\n",
    "        this_df.index = pd.MultiIndex.from_product(\n",
    "            [[loc], [tx], list(start_times[loc][tx].index)],\n",
    "            names=[\"location\", \"group\", \"module\"],\n",
    "        )\n",
    "        t_mod_df.loc[idx[loc, tx, :], :] = this_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mod_df.to_parquet(s.module_temps_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
